question_number,skill_name,question_text,option_1,option_2,option_3,option_4,correct_option
1,Why Data is Important,"Why is data often referred to as the ""fuel"" of machine learning models?",It suggests that model performance depends on data quality and quantity.,It means data can be replaced like engine oil after each run.,It implies models cannot function without a computer.,It indicates data is only needed during training.,1
2,Why Data is Important,"Which phrase best captures the idea of ""garbage in, garbage out"" in machine learning?",Bad or irrelevant input data leads to bad model predictions.,Machine learning algorithms automatically clean the data.,No data is needed for a model to make accurate predictions.,Data garbage can be turned into valuable output.,1
3,Why Data is Important,"If a house price prediction model is trained on data with missing important features (like house size), what is likely to happen?",The model's accuracy will likely decrease due to incomplete information.,The model will become more accurate by ignoring the missing features.,The model's performance will remain unaffected.,The model will automatically fill in missing values without impact.,1
4,Why Data is Important,Which of the following is NOT typically considered a characteristic of high-quality data for machine learning?,Accuracy,Consistency,Encryption,Timeliness,3
5,Why Data is Important,What is the general effect of increasing the amount of good training data on a machine learning model?,It tends to improve the model's performance.,It always causes the model to overfit.,It decreases the model's accuracy by adding noise.,It has no effect once a certain data size is reached.,1
6,Statistical Learning vs Machine Learning,"What is a primary focus of statistical learning, compared to machine learning?",Understanding and modeling relationships between variables.,Maximizing predictive accuracy without regard to interpretability.,Using only neural networks for data analysis.,Applying only unsupervised techniques.,1
7,Statistical Learning vs Machine Learning,Which best describes the focus of machine learning compared to statistical learning?,Discovering patterns in large datasets and making predictions.,Formulating detailed hypotheses without using data.,Building models exclusively for inference and explanation.,Ignoring large data in favor of theoretical analysis.,1
8,Statistical Learning vs Machine Learning,Statistical learning often begins with a hypothesis about the data structure. Which example illustrates this approach?,Assuming shoe size is related to height and modeling that relationship.,Randomly guessing outputs for a neural network.,Clustering data points into groups without labels.,Training a neural network with no initial assumptions.,1
9,Statistical Learning vs Machine Learning,Which statement best contrasts the objectives of machine learning and statistical inference (statistical learning)?,"Machine learning often uses complex models to maximize accuracy, while inference uses simpler models to understand relationships.","Machine learning does not use any data for learning, whereas statistical inference uses only data.",There is no difference; both use exactly the same techniques.,"Statistical inference ignores relationships in data, while machine learning models everything.",1
10,Statistical Learning vs Machine Learning,Which scenario is more characteristic of statistical learning rather than machine learning?,Analyzing how specific input variables influence an outcome through interpretable models.,Training a large neural network on images to maximize classification accuracy.,Clustering millions of unlabeled data points into groups.,Automating hyperparameter tuning for a deep model.,1
11,Predictor and Response Variables,What are predictor variables (also called features) in a supervised learning context?,Input variables (independent variables) that are used to predict the response.,Output variables that we try to predict.,Error terms or residuals.,Model parameters.,1
12,Predictor and Response Variables,What is another name for the response variable in regression?,Dependent variable (target),Predictor variable,Noise term,Covariate,1
13,Predictor and Response Variables,"In the equation Y = f(X) + ε, what does X represent?",The set of predictor variables (features).,The random error term.,The predicted output.,The residuals.,1
14,Predictor and Response Variables,"In a housing price dataset where X contains features (like square footage and number of bedrooms) and Y is the price, what roles do X and Y play?","X are predictors (features), Y is the response (target).","X are responses, Y are predictors.",X and Y are both predictors.,Neither X nor Y is a predictor or response.,1
15,Predictor and Response Variables,"For predicting student exam scores, which of the following would NOT be a predictor variable?",Exam score,Hours studied,Class attendance,IQ test score,1
16,Noise and Irreducible Error,What does irreducible error (often called noise) refer to in a model Y = f(X) + ε?,Random variability in Y that cannot be explained by the predictors X.,The error due to using a wrong model.,Error that can be eliminated by choosing a better algorithm.,Variance in model parameters due to limited data.,1
17,Noise and Irreducible Error,Which statement about irreducible error is true?,It cannot be reduced by any model or additional data; it is inherent in the problem.,It can be eliminated by adding more features.,It is the same as model bias.,It only occurs in classification problems.,1
18,Noise and Irreducible Error,"In the equation Y = f(X) + ε, what role does ε (epsilon) play?",It represents the irreducible error (random noise) in the response.,It is the predicted value of Y.,It is the training set error.,It is the model’s prediction function.,1
19,Noise and Irreducible Error,What happens to the irreducible error if we increase model complexity or add more data?,It remains unchanged.,It decreases to zero.,It always increases.,It becomes negative.,1
20,Noise and Irreducible Error,Which of the following is NOT a typical source of irreducible error?,Natural randomness in the data,Measurement errors in variables,Missing relevant features,Choosing the wrong algorithm,4
21,Reducible vs Irreducible Error,Which type of error in a prediction problem can be reduced by improving the model or getting more data?,Reducible error,Irreducible error,Noise error,None of the above,1
22,Reducible vs Irreducible Error,Which error remains even if the model perfectly matches the true function f?,Irreducible error,Reducible error,Training error,Validation error,1
23,Reducible vs Irreducible Error,What does reducible error consist of?,The error from approximating the true function f with f_hat,The inherent noise in the data,The total error,None of the above,1
24,Reducible vs Irreducible Error,Which statement correctly contrasts reducible and irreducible error?,Reducible error is due to model imperfection; irreducible error is due to inherent noise.,Irreducible error can be reduced by better algorithms.,Reducible error includes noise variance.,Reducible error cannot be reduced.,1
25,Reducible vs Irreducible Error,"As we collect more data and improve the model, which error component is most likely to decrease?",Reducible error,Irreducible error,Intrinsic noise,Measurement error,1
26,Accuracy vs Interpretability,What trade-off is typically observed between model accuracy and interpretability?,More complex models often have higher accuracy but lower interpretability.,More complex models always have higher interpretability.,Interpretability has no effect on accuracy.,Simpler models are always more accurate.,1
27,Accuracy vs Interpretability,Which model type is generally considered more interpretable?,Simple linear regression,Deep neural network,K-means clustering,Random forests (without constraints),1
28,Accuracy vs Interpretability,What does it mean for a model to be interpretable?,Its predictions or structure can be easily understood by humans.,It always has higher accuracy than other models.,It is implemented in simple code.,It does not use any training data.,1
29,Accuracy vs Interpretability,Prioritizing interpretability over accuracy in model selection might involve which choice?,Using a simple linear model even if it is slightly less accurate.,Using a complex deep learning model for maximum accuracy.,Ignoring the interpretability of the model completely.,Choosing a model at random.,1
30,Accuracy vs Interpretability,Why might a doctor prefer a slightly less accurate but interpretable model in healthcare?,To understand and trust how the model makes decisions.,Because interpretability always increases accuracy.,Because interpretability reduces training data requirements.,No reason; accuracy is all that matters.,1
31,Regression vs Classification,What is the main difference between regression and classification tasks in supervised learning?,"Regression predicts continuous outcomes, while classification predicts discrete categories.","Regression uses unlabeled data, classification uses labeled data.",Regression is used only for linear models.,Classification always uses more features.,1
32,Regression vs Classification,Which of the following problems is a classification task?,Determining whether an email is spam or not spam.,Predicting tomorrow's temperature.,Estimating a person's income.,Predicting the height of a plant.,1
33,Regression vs Classification,Which of the following is an example of a regression problem?,Predicting house prices based on features.,Classifying images of cats versus dogs.,Sorting emails into folders.,Predicting whether a transaction is fraudulent.,1
34,Regression vs Classification,When should you use regression instead of classification?,When the target variable is a continuous numeric value.,When the target variable is a category label.,When there is no data available.,When the data is unlabeled.,1
35,Regression vs Classification,"In a classification problem, the response (target) variable is typically which of the following types?",Discrete class labels,Continuous numeric values,Probability distributions,Complex data structures,1
36,Unsupervised and Semi-Supervised Learning,What is unsupervised learning?,Learning from data without labels to discover patterns or structure.,Learning from labeled data to predict outcomes.,Learning where only some labels are available.,Learning with continuous output variables.,1
37,Unsupervised and Semi-Supervised Learning,What is semi-supervised learning?,Learning that uses a small amount of labeled data along with a large amount of unlabeled data.,Learning with only labeled data.,Learning without any labels at all.,A form of reinforcement learning.,1
38,Unsupervised and Semi-Supervised Learning,Which of the following is typically an unsupervised learning task?,Clustering data points into groups based on similarity.,Classifying images with labels.,Predicting house prices from features.,Training a model on labeled training data.,1
39,Unsupervised and Semi-Supervised Learning,When is semi-supervised learning most useful?,When you have a large amount of unlabeled data and only a small labeled dataset.,When all data is labeled with no missing labels.,When you have no data at all.,When the data is already perfectly labeled.,1
40,Unsupervised and Semi-Supervised Learning,What is a key advantage of semi-supervised learning?,It can improve model performance by leveraging unlabeled data.,It does not require any labeled data.,It always outperforms supervised learning.,It avoids any need for model evaluation.,1
41,Reinforcement Learning Overview,What characterizes reinforcement learning (RL) as a learning paradigm?,An agent learns to make decisions by interacting with an environment and receiving rewards or penalties.,Learning from a fixed labeled dataset.,Clustering data without any labels.,Learning without any form of feedback.,1
42,Reinforcement Learning Overview,"In reinforcement learning, learning is primarily driven by what?",Feedback in the form of rewards and penalties from the environment.,Explicit labels provided in the data.,Unsupervised pattern discovery.,Random guess and error.,1
43,Reinforcement Learning Overview,Which of the following scenarios is an example of a reinforcement learning problem?,A robot learning to navigate a maze by trial and error to maximize reward.,Classifying emails into spam or not spam using labeled data.,Grouping customers into segments without using labels.,Predicting stock prices using historical data.,1
44,Reinforcement Learning Overview,What is the primary goal of the agent in reinforcement learning?,To maximize its cumulative reward over time.,To minimize the training error.,To cluster data points into homogeneous groups.,To solve a static optimization problem.,1
45,Reinforcement Learning Overview,Which statement distinguishes reinforcement learning from supervised learning?,Reinforcement learning learns from interaction and feedback rather than from a pre-labeled dataset.,Reinforcement learning always requires more data than supervised learning.,Reinforcement learning does not use any models.,Reinforcement learning is only used for classification tasks.,1
46,Bias,"In the context of the bias-variance tradeoff, what is model bias?","The error due to incorrect assumptions in the learning algorithm, causing systematic deviation from the true function.",Random fluctuations in the training data.,The difference between training and test error.,The amount of data bias introduced during data collection.,1
47,Bias,High bias in a model typically leads to which problem?,Underfitting the training data (poor accuracy on both training and test).,"Overfitting the training data (low training error, high test error).",Excessive variance in predictions.,Data leakage.,1
48,Bias,Which of the following models is likely to have high bias?,"A very simple model (e.g., a linear model on highly nonlinear data).","A very complex model (e.g., deep neural network on any data).",The true model (when known).,Random guessing.,1
49,Bias,What does high bias indicate about a model's predictions?,They consistently deviate from the true values due to oversimplification of the model.,They have high variance but no pattern.,They are random noise.,They perfectly match the data.,1
50,Bias,High bias in a model usually results from:,Using an overly simple model for the complexity of the data.,Using a large number of complex features.,Ensembling multiple models.,Collecting more data.,1
51,Variance,"In the bias-variance tradeoff, what is model variance?",The variability of the model’s predictions across different training sets.,The difference between training and test errors.,The bias introduced by incorrect model assumptions.,The amount of random noise in the data.,1
52,Variance,High variance in a model typically leads to which problem?,Overfitting the training data (good training accuracy but poor test accuracy).,Underfitting the data (poor training accuracy).,No change in performance.,Guaranteed low error on test data.,1
53,Variance,Which model is likely to have high variance?,"A very complex model (e.g., deep neural network) especially with limited data.","A very simple model (e.g., a single linear regression line).",The true underlying function (if known).,A model that is deterministic and fixed.,1
54,Variance,Which statement is true about variance in model error?,Adding more training data generally helps to reduce variance.,High variance means the model's predictions are stable across different datasets.,Variance is unaffected by model complexity.,Variance only occurs in classification.,1
55,Variance,Which strategy can help reduce a model's variance?,Regularization or using a simpler model.,Adding more high-degree polynomial features without regularization.,Increasing the model complexity further.,Ignoring bias altogether.,1
56,Bias-Variance Decomposition,What are the components of the bias-variance decomposition of expected prediction error?,Bias^2 + Variance + Irreducible error (noise),Bias + Variance only,Training error + Test error,Model error + Data error,1
57,Bias-Variance Decomposition,"If a model has very high bias, what can we say about its variance?",The variance is likely low (the model is simple and stable).,The variance is likely high.,The variance must be zero.,Variance is unrelated to bias.,1
58,Bias-Variance Decomposition,"As model complexity increases, what happens to bias and variance (in general)?",Bias decreases and variance increases.,Bias increases and variance decreases.,Both bias and variance decrease.,Neither bias nor variance changes.,1
59,Bias-Variance Decomposition,What is the bias-variance tradeoff?,Decreasing bias typically increases variance and vice versa.,Bias and variance are the same thing.,You can minimize both bias and variance simultaneously by any means.,It refers to the balance between training time and test accuracy.,1
60,Bias-Variance Decomposition,"Which scenario exemplifies a low-bias, high-variance model?",A highly complex model that fits the training data very well but performs poorly on new data.,A very simple model that underfits the data.,A model that performs perfectly on all datasets.,A model with no parameters.,1
61,Training vs Test Error,What is training error in machine learning?,The error (or loss) the model makes on the training dataset.,"The error the model makes on new, unseen data.",The difference between actual and predicted values for one example.,The variance of model predictions.,1
62,Training vs Test Error,What is test (or generalization) error?,"The error the model makes on new, unseen data (validation or test set).",The error on the training set.,The error within the model parameters.,The noise in the training data.,1
63,Training vs Test Error,Which is typically smaller for a given model?,Training error is typically smaller than test error.,Test error is typically smaller than training error.,They are always equal.,They cannot be compared.,1
64,Training vs Test Error,Which error metric is more important for understanding how a model will perform in the real world?,Test (generalization) error on unseen data.,Training error on the training data.,The difference between training and test error.,The absolute sum of residuals.,1
65,Training vs Test Error,"If a model has very low training error but high test error, what issue does this indicate?",Overfitting to the training data.,Underfitting to the training data.,Perfect model generalization.,Data leakage.,1
66,Overfitting vs Underfitting,What is overfitting in machine learning?,When a model fits the training data too closely and fails to generalize to new data.,When a model is too simple and performs poorly on training data.,When the model has high bias and low variance.,When the test error is lower than the training error.,1
67,Overfitting vs Underfitting,What is underfitting in machine learning?,When a model is too simple to capture the underlying pattern and performs poorly on both training and test data.,When a model fits the training data perfectly.,When the model has high variance and low bias.,When it uses too many features.,1
68,Overfitting vs Underfitting,Which pattern of errors typically indicates overfitting?,Very low training error but much higher test error.,High training error and high test error.,Equal training and test errors.,No training data error.,1
69,Overfitting vs Underfitting,Which pattern of errors typically indicates underfitting?,High training error and similar high test error.,Low training error and high test error.,No training error and no test error.,One high error and one low error arbitrarily.,1
70,Overfitting vs Underfitting,Which technique is often used to reduce overfitting in a model?,Applying regularization or simplifying the model.,Adding more features with no consideration for redundancy.,Increasing model complexity significantly.,Using the model less.,1
71,Validation Set Approach,What is the purpose of using a validation set in machine learning?,To tune model hyperparameters and estimate performance on unseen data.,To serve as the final test set for deployment.,To increase the training set size.,To label unlabeled data.,1
72,Validation Set Approach,How is a validation set typically created?,By splitting the original training data into a separate validation subset and a smaller training subset.,By using the test set and calling it validation.,By randomly generating new data unrelated to the problem.,By combining multiple datasets.,1
73,Validation Set Approach,What is a drawback of using a single validation set approach?,The performance estimate can be highly dependent on how the data is split (high variance).,It always gives a perfect estimate of test error.,It eliminates any need for a test set.,It causes overfitting by definition.,1
74,Validation Set Approach,"In the validation set approach, what is a common use of the validation data?",Selecting the best model or hyperparameters based on its performance.,Training the final model to convergence.,Collecting more data.,Labeling the training data.,1
75,Validation Set Approach,Which of the following is NOT an advantage of the simple validation set approach?,It uses all data for training without holding any out.,It provides a quick estimate of generalization error.,It helps in choosing hyperparameters.,It avoids optimistic bias from using test data.,1
76,K-Fold Cross Validation,How does k-fold cross-validation work in evaluating a model?,"The data is split into k folds, and the model is trained k times each time leaving out one distinct fold for validation.",The data is used once for training and once for testing without splits.,The data is shuffled and split into two equal parts only once.,All data is used for training and testing simultaneously.,1
77,K-Fold Cross Validation,What is one advantage of k-fold cross-validation over a single train/test split?,It provides a more reliable estimate of model performance with less dependence on a particular random split.,It always produces the same results as a single split.,It uses no additional computation.,It always uses 100% of data as test data.,1
78,K-Fold Cross Validation,"In k-fold cross-validation, each observation is used for testing exactly how many times?",Once (in one of the k folds).,k times (every fold).,Zero times (none of the folds).,Half of the folds.,1
79,K-Fold Cross Validation,What does 'k' represent in k-fold cross-validation?,The number of folds or splits of the dataset.,The number of features in the data.,The number of classes to predict.,The number of layers in the model.,1
80,K-Fold Cross Validation,Which of the following is a common choice for k in k-fold cross-validation?,"5 or 10 folds (e.g., 5-fold or 10-fold CV).",k equal to the number of classes.,k equal to 2 (two folds).,k equal to the number of features.,1
81,LOOCV,What does Leave-One-Out Cross-Validation (LOOCV) involve?,"Training the model on all but one observation and testing on the single left-out one, repeated for each observation.",Dividing the data in half one time for training and testing.,Leaving out a random 10% of the data for validation each time.,Using bootstrapping instead of splitting.,1
82,LOOCV,What is a major disadvantage of LOOCV compared to k-fold cross-validation with k< n?,It is computationally expensive and can lead to higher variance of the estimate.,It always underestimates the true error.,It does not use the data efficiently.,It eliminates the need for any test data.,1
83,LOOCV,When might one prefer to use LOOCV?,"When the dataset is very small, to make use of as much data as possible for training.",When there is plenty of data and computation is not an issue.,When the target variable is categorical.,When using time series data.,1
84,LOOCV,LOOCV is a special case of k-fold cross-validation. What is the value of k in LOOCV if there are n observations?,k = n (each fold consists of one observation).,k = 2 (two folds of half data each).,"k = 1 (no folds, just one group).",k = 10 (fixed 10 folds).,1
85,LOOCV,"Compared to using a single train/test split, LOOCV tends to produce:",An estimate that can have high variance but uses almost all data for training.,A lower variance and lower bias estimate.,No estimate at all.,The same estimate always.,1
86,Fairness in ML,What is the goal of fairness in machine learning?,To ensure models do not systematically discriminate against protected or sensitive groups.,To maximize overall accuracy regardless of group outcomes.,To randomize decisions to avoid bias.,To ignore all categorical variables.,1
87,Fairness in ML,Which of the following is considered a protected or sensitive attribute in fairness contexts?,Race or gender.,Feature scaling factor.,Random noise variable.,Prediction score.,1
88,Fairness in ML,Which practice can help promote fairness in a model?,Removing or not using protected attributes in model training.,Maximizing accuracy only for the majority class.,Adding more hidden layers to a neural network.,Augmenting data with random noise.,1
89,Fairness in ML,Algorithmic bias can occur when a model unfairly treats certain groups. Which of the following is a potential cause of such bias?,Training data that reflects historical prejudices or disparities.,Using too many features in a model.,Normalizing all numerical variables.,Cross-validation splitting.,1
90,Fairness in ML,Why is fairness an important consideration in machine learning models?,To avoid perpetuating or amplifying existing societal inequalities in automated decision-making.,To reduce the training time of the model.,To guarantee zero classification error.,To ensure data privacy.,1
91,Feedback Loops and Societal Biases,What is a feedback loop in the context of machine learning and societal impact?,"When model predictions influence future data, potentially reinforcing biases over time.",When a model uses feedback from users to retrain itself.,When the same data point is used multiple times in training.,When labels are fed back into the input features.,1
92,Feedback Loops and Societal Biases,Which of the following is an example of a feedback loop in a predictive policing system?,"Sending more police to neighborhoods the model predicts as high-crime, leading to more arrest data from those areas.",Randomly shuffling crime data.,Using a static crime dataset for training once.,Predicting weather instead of crime.,1
93,Feedback Loops and Societal Biases,What is societal bias in machine learning?,Bias in model predictions that arise from historical or cultural prejudices reflected in the data.,The randomness in societal data.,Bias that only occurs in image data.,Errors introduced by data encryption.,1
94,Feedback Loops and Societal Biases,How can algorithmic feedback loops affect a deployed model?,They can amplify existing biases by reinforcing the model's previous predictions in new data.,They always correct past biases automatically.,They make the model ignore new data.,They have no impact on the model.,1
95,Feedback Loops and Societal Biases,Why must we consider societal biases when using machine learning in decision-making?,Because models trained on biased data can perpetuate or worsen unfair outcomes for certain groups.,Because bias always improves model accuracy.,Because societal bias has no relation to model performance.,Because bias is required for generalization.,1
96,Simple Linear Regression,What defines a simple linear regression model?,It models the relationship between one input (predictor) and one output (response) using a straight line.,It models multiple inputs with one output using a complex curve.,It groups data into clusters based on similarity.,It predicts categories using probabilities.,1
97,Simple Linear Regression,"In a simple linear regression model Y = β0 + β1X, what does β1 represent?",The average change in Y for a one-unit increase in X.,The intercept of the y-axis.,The error term of the model.,The number of data points.,1
98,Simple Linear Regression,Which of the following is NOT an assumption of simple linear regression?,The predictor X is normally distributed.,The errors are independent and normally distributed with constant variance.,The relationship between X and Y is linear.,The observations are independent.,1
99,Simple Linear Regression,What does the Ordinary Least Squares (OLS) method do when fitting a linear regression line?,It minimizes the sum of squared differences between observed and predicted Y values.,It maximizes the number of data points perfectly fitted.,It ignores the residuals during fitting.,It uses the median instead of the mean.,1
100,Simple Linear Regression,How many parameters are estimated in a simple linear regression with one predictor?,Two parameters (slope and intercept).,One parameter (slope only).,"Three parameters (slope, intercept, variance).",It depends on the number of data points.,1
101,Multiple Linear Regression,What is the main difference between simple and multiple linear regression?,Multiple linear regression uses more than one predictor variable.,Multiple regression always uses a polynomial.,Simple regression can only use categorical data.,They are the same; there is no difference.,1
102,Multiple Linear Regression,"In multiple linear regression, what does the coefficient β_j (for predictor X_j) represent?","The effect on Y of a one-unit increase in X_j, holding all other predictors constant.",The total variance explained by X_j.,The correlation between X_j and Y.,The intercept of the model.,1
103,Multiple Linear Regression,Which of the following issues is unique to multiple regression and not present in simple regression?,Multicollinearity (high correlation between predictors).,Linearity assumption between X and Y.,Homoscedasticity of residuals.,Independence of observations.,1
104,Multiple Linear Regression,What does the R-squared metric represent in multiple regression?,The proportion of variance in Y explained by the model's predictors.,The number of predictors used.,The prediction error of the model.,The average of residuals.,1
105,Multiple Linear Regression,What is the purpose of adjusted R-squared in multiple regression?,To provide a measure of model fit that accounts for the number of predictors used.,To adjust the data before regression.,To regularize the regression coefficients.,To transform predictors to orthogonal basis.,1
106,Least Squares Estimation,What is the goal of least squares estimation in linear regression?,To find parameter values that minimize the sum of squared residuals (errors).,To maximize the sum of absolute residuals.,To find parameters that maximize the likelihood with no regard to errors.,To minimize the number of predictors.,1
107,Least Squares Estimation,Which equation gives the ordinary least squares solution for the coefficients β in multiple regression?,β̂ = (X^T X)^{-1} X^T y,β̂ = X X^T y,β̂ = X^T y,β̂ = (X X^T) y,1
108,Least Squares Estimation,What condition on the design matrix X ensures that the least squares solution is unique?,X must have full column rank (no perfect multicollinearity).,X must be square.,X must be orthogonal.,X must have more columns than rows.,1
109,Least Squares Estimation,"In ordinary least squares regression with an intercept, what is a property of the residuals?",The residuals sum to zero.,The residuals are all equal.,The residuals sum to one.,The residuals are constant.,1
110,Least Squares Estimation,"Under classical linear regression assumptions (Gauss-Markov), what property does the OLS estimator have?",It is the Best Linear Unbiased Estimator (BLUE).,It always has zero variance.,It always perfectly predicts the data.,It requires no assumptions.,1
111,Interaction Terms,What is an interaction term in a regression model?,A term formed by multiplying two predictor variables to capture their combined effect.,A variable that interacts with the error term.,The error term of the regression.,An output variable in a system of equations.,1
112,Interaction Terms,Why might you include an interaction term between X1 and X2 in a regression model?,To model a situation where the effect of X1 on Y depends on the value of X2.,To linearize a nonlinear relationship.,To ensure the residuals have constant variance.,To eliminate multicollinearity.,1
113,Interaction Terms,Which of the following indicates a significant interaction between X1 and X2?,The coefficient for the product term X1*X2 is significantly different from zero.,The coefficient for X1 is zero.,The p-value for X1 is high.,X1 and X2 are uncorrelated.,1
114,Interaction Terms,Which of the following is an example of an interaction term?,Temperature * Pressure,Temperature + Pressure,Temperature - Pressure,Temperature,1
115,Interaction Terms,"If a model without an interaction does not fit well, what is one possible solution?",Include an interaction term to allow combined effects of predictors.,Remove all predictors and use only the intercept.,Add more data without changing the model form.,Ignore the issue.,1
116,Collinearity and VIF,What is collinearity (or multicollinearity) in regression analysis?,A situation where two or more predictor variables are highly correlated with each other.,A situation where a predictor is exactly equal to the response.,The presence of outliers in the data.,A model that has too many predictors.,1
117,Collinearity and VIF,Why is collinearity problematic in a regression model?,It makes coefficient estimates unstable and inflates their variances.,It causes the model to become linear.,It always reduces predictive accuracy.,It prevents calculation of R-squared.,1
118,Collinearity and VIF,What does the Variance Inflation Factor (VIF) measure?,How much the variance of a coefficient is inflated due to collinearity with other predictors.,The amount of variance explained by a single predictor.,The increase in R-squared when adding a new predictor.,The variance of the dependent variable.,1
119,Collinearity and VIF,A VIF value greater than 10 (or sometimes 5) typically indicates:,A problematic level of multicollinearity among predictors.,That the predictor is not important.,Perfect prediction by the model.,No correlation between predictors.,1
120,Collinearity and VIF,How can you address multicollinearity in a regression model?,"Remove or combine correlated predictors, or use dimensionality reduction.",Always increase the model complexity.,Ensure the model has an intercept.,There is no need to address it.,1
121,Heteroscedasticity and Residual Analysis,What is heteroscedasticity in the context of regression residuals?,When the variance of the residuals is not constant across all levels of the predictors.,When residuals are exactly zero.,When residuals are normally distributed.,When there are no outliers.,1
122,Heteroscedasticity and Residual Analysis,Why is heteroscedasticity a problem in linear regression?,"It violates the assumption of constant variance, which can lead to inefficient estimates of standard errors and confidence intervals.",It causes the regression line to be nonlinear.,It makes the residuals independent.,It is only a problem for logistic regression.,1
123,Heteroscedasticity and Residual Analysis,Which pattern in a plot of residuals versus fitted values suggests heteroscedasticity?,A funnel or cone shape (variance of residuals increasing or decreasing with fitted values).,Random scatter with uniform spread.,A perfect horizontal line.,Only positive residuals.,1
124,Heteroscedasticity and Residual Analysis,What is the purpose of residual analysis in regression?,"To check model assumptions (linearity, normality, homoscedasticity, etc.) by examining residual patterns.",To train the model further.,To reduce the number of predictors.,To transform dependent variable.,1
125,Heteroscedasticity and Residual Analysis,Which technique can be used to address heteroscedasticity in a regression model?,"Using weighted least squares or transforming the dependent variable (e.g., log transformation).",Increasing the number of predictors.,Dropping the intercept from the model.,Using a larger learning rate.,1
126,Comparison: Linear Regression vs KNN,What is a key difference between linear regression and k-nearest neighbors (KNN) regression?,"Linear regression is a parametric global model, while KNN is a non-parametric local model.","KNN always provides a linear decision boundary, whereas linear regression provides a curved boundary.","Linear regression does not require training, while KNN requires extensive training.",There is no difference; they are the same method.,1
127,Comparison: Linear Regression vs KNN,Which model is generally more interpretable?,Linear regression (because its coefficients have clear meaning).,K-nearest neighbors (because it memorizes data).,Both are equally interpretable.,Neither is interpretable.,1
128,Comparison: Linear Regression vs KNN,Which method is better suited for capturing highly nonlinear relationships in data?,"K-nearest neighbors (KNN) regression, because it makes no global assumptions.","Linear regression, because it can fit any curve.",They are equally good for nonlinear data.,Neither can model nonlinear relationships.,1
129,Comparison: Linear Regression vs KNN,How does K-nearest neighbors (KNN) regression make predictions for a new point?,It averages the target values of the k closest training points to the new point.,It uses a fixed linear equation.,It chooses the maximum target value in the training set.,It draws a new random sample.,1
130,Comparison: Linear Regression vs KNN,What effect does the curse of dimensionality have on KNN regression?,KNN performance typically degrades in high-dimensional feature spaces.,It makes KNN always better than linear regression.,It does not affect KNN at all.,It makes KNN faster.,1
131,Curse of Dimensionality,"What is the ""curse of dimensionality""?",The various problems that arise when the number of features (dimensions) is very large compared to the number of observations.,A situation where adding more dimensions always reduces prediction error.,A technique for dimensionality reduction.,A name for high computation time of any algorithm.,1
132,Curse of Dimensionality,How does high dimensionality affect data and models?,"Data points become very sparse, and many algorithms require exponentially more data to perform well.",It makes data more dense and easier to model.,All algorithms become linear.,It only affects visualization.,1
133,Curse of Dimensionality,Which type of model is particularly affected by the curse of dimensionality?,Non-parametric methods like KNN and kernel methods.,Simple linear regression with one feature.,Models that do not use distance metrics.,Decision trees (with small depth).,1
134,Curse of Dimensionality,What is one way to mitigate the curse of dimensionality?,"Dimensionality reduction (e.g., PCA) or feature selection.",Increasing the number of dimensions even more.,Using no data at all.,Always normalizing features.,1
135,Curse of Dimensionality,What happens to the notion of distance between points as dimensionality increases?,"All points tend to appear similarly distant, making nearest-neighbor methods less effective.",Distances become more meaningful and discriminative.,Distances become negative.,It has no effect.,1
136,Basis Expansions,What is a basis expansion in regression modeling?,"Transforming original predictors into a set of basis functions (e.g., polynomial terms) to allow more flexible relationships.",Reducing the number of features by PCA.,Using only the original features in the model.,Clustering the data before regression.,1
137,Basis Expansions,Why might you use basis expansions (like polynomial terms) in a model?,To model nonlinear relationships while still using a linear model in the expanded space.,To ensure the model remains purely linear.,To reduce model complexity.,To enforce monotonic relationships.,1
138,Basis Expansions,Which of the following is an example of a basis function used in an expansion?,x^2 (a polynomial feature),x itself (original feature),Random noise variable,The mean of x,1
139,Basis Expansions,"What is a potential drawback of using many basis expansion terms (e.g., high-degree polynomials)?",It can lead to overfitting and high model complexity.,It guarantees better generalization.,It reduces the number of parameters to estimate.,It always makes the model underfit.,1
140,Basis Expansions,How do basis expansions relate to model interpretability?,They can make the model less interpretable because the relationship between original features and output is more complex.,They always increase interpretability.,They have no effect on interpretability.,They simplify the model.,1
141,Ridge Regression and Lasso,What does ridge regression do differently than ordinary least squares?,It adds an L2 penalty (sum of squared coefficients) to the loss function to shrink coefficients.,It adds an L1 penalty to the loss function.,It uses only one predictor.,It does not use any data.,1
142,Ridge Regression and Lasso,What is the key feature of lasso regression compared to ridge?,Lasso uses an L1 penalty and can set some coefficients exactly to zero (feature selection).,Lasso uses an L2 penalty.,Lasso ignores the loss function.,Lasso always increases R-squared.,1
143,Ridge Regression and Lasso,Which statement is true?,"Ridge tends to shrink coefficients continuously, while lasso can force some coefficients to be zero.",Ridge always sets coefficients to zero.,Lasso adds more features automatically.,Ridge and lasso behave exactly the same.,1
144,Ridge Regression and Lasso,How do ridge and lasso differ in their effect on model bias and variance?,Both increase bias and decrease variance by regularizing the solution.,"Ridge increases variance, lasso decreases bias.","Ridge and lasso only affect training error, not bias/variance.",Ridge decreases bias and lasso increases variance.,1
145,Ridge Regression and Lasso,Which method would you choose if you suspect many irrelevant features in your model?,"Lasso regression, because it can perform feature selection by zeroing out coefficients.","Ridge regression, because it sets all coefficients equally.","Ordinary least squares, because it is simplest.","KNN regression, because it is non-parametric.",1
146,Piecewise Polynomials,What is a piecewise polynomial regression model?,A model that fits different polynomial functions in different intervals of the predictor.,A model that uses only linear terms.,A clustering algorithm for regression.,A polynomial model with no breaks.,1
147,Piecewise Polynomials,Why use piecewise polynomial models instead of a single high-degree polynomial?,To capture complex relationships while avoiding extreme oscillations over the entire domain.,To ensure a single global fit with no flexibility.,To reduce the number of parameters to one.,To enforce linear relationships.,1
148,Piecewise Polynomials,"In a piecewise polynomial model, what is a 'knot'?",A point in the predictor space where the polynomial pieces meet.,The degree of the polynomial.,The slope of the regression line.,A type of residual.,1
149,Piecewise Polynomials,What does it mean for a piecewise polynomial to be 'continuous' at the knots?,"The different polynomial pieces have the same function value at the knot, so the transition has no jump.",The pieces have the same slope at the knot.,The model has no polynomial terms.,The knot values are identical for all data points.,1
150,Piecewise Polynomials,What can happen if a piecewise polynomial model is not constrained to be continuous at the knots?,The fitted function may have jumps or discontinuities at the knot points.,The model will have fewer parameters.,There will be no effect on the fit.,The residuals will sum to zero automatically.,1
151,Natural and Smoothing Splines,What is a natural spline?,A spline that is constrained to be linear beyond the boundary knots.,A spline with no constraints at the ends.,A polynomial of degree one.,A clustering algorithm.,1
152,Natural and Smoothing Splines,What is a smoothing spline?,A nonparametric regression method that minimizes a tradeoff between fit to data and smoothness (penalizes roughness).,A spline that interpolates all data points exactly.,A spline with no penalty term.,A linear spline of low degree.,1
153,Natural and Smoothing Splines,Which statement is true about natural and smoothing splines?,Natural splines impose linear behavior at the boundaries; smoothing splines include a penalty parameter to control smoothness.,Natural splines always interpolate the data; smoothing splines do not use data.,Both are types of parametric linear regression.,They are the same thing.,1
154,Natural and Smoothing Splines,What role does the smoothing parameter (lambda) play in a smoothing spline?,It controls the tradeoff between the fidelity to the data and the smoothness of the function.,It selects which variables to include.,It sets the degree of the polynomial exactly.,It normalizes the features.,1
155,Natural and Smoothing Splines,Which of the following is an advantage of smoothing splines over high-degree polynomials?,"They avoid wild oscillations by penalizing rapid changes, leading to a smoother fit.",They guarantee a perfect fit to all data.,They require no tuning parameters.,They are linear.,1
156,Kernel Smoothing Methods,What is kernel smoothing (kernel regression)?,"A nonparametric method where the prediction is a weighted average of nearby observations, with weights given by a kernel function.",A way to fit a polynomial globally to the data.,A parametric method using decision trees.,A method of clustering data points.,1
157,Kernel Smoothing Methods,What does the bandwidth parameter control in kernel smoothing?,The width of the kernel window (degree of smoothing).,The number of neighbors to consider.,The polynomial degree.,The number of clusters.,1
158,Kernel Smoothing Methods,Which of the following is a common kernel function used in kernel smoothing?,Gaussian (normal) kernel.,Euclidean distance.,Polynomial expansion.,Histogram binning.,1
159,Kernel Smoothing Methods,What is the effect of using a very small bandwidth in kernel smoothing?,"The fit will be very wiggly (high variance, low bias).","The fit will be very smooth (low variance, high bias).",The kernel will ignore nearby points.,It makes the method parametric.,1
160,Kernel Smoothing Methods,Kernel smoothing is an example of which type of modeling approach?,Nonparametric (it does not assume a fixed parametric form).,Parametric (with fixed parameters).,Semi-supervised (mix of labeled and unlabeled).,Reinforcement learning.,1
161,Local Linear Regression,What is local linear regression (also known as locally weighted regression)?,"A method that fits a weighted linear model around each target point to make predictions, improving edge behavior.",A global linear model fit to all data.,A neural network trained on local data.,A type of clustering algorithm.,1
162,Local Linear Regression,How does local linear regression improve upon simple kernel smoothing (Nadaraya-Watson)?,It reduces bias near the boundaries by fitting a local line rather than just a weighted average.,It requires no tuning parameters.,It always fits the data perfectly.,It is the same as linear regression.,1
163,Local Linear Regression,"In local linear regression, predictions at a point x0 are obtained by:",Fitting a linear regression to data near x0 using kernel weights and evaluating that line at x0.,Averaging all y values from the entire dataset.,Fitting a polynomial globally.,Taking the median of local y values.,1
164,Local Linear Regression,Which statement about local linear regression is true?,It is a nonparametric technique that allows the estimated slope to change with x.,It fits a single straight line to all data.,It cannot handle boundaries well.,It does not use kernel functions.,1
165,Local Linear Regression,Local linear regression is particularly useful when:,The relationship between predictors and response is nonlinear and we want smooth but flexible fits.,The data is perfectly linear.,We only have one data point.,We want a global parametric model.,1
166,Bandwidth and Kernel Choice,What effect does choosing a larger bandwidth have in kernel smoothing?,"It produces a smoother (more biased, less variable) fit.","It makes the fit more wiggly (less biased, more variable).",It has no effect on the fit.,It always makes the model linear.,1
167,Bandwidth and Kernel Choice,Which of the following is an example of a kernel function used in smoothing methods?,Gaussian kernel,Squared Euclidean distance,Polynomial regression,Logarithmic scaling,1
168,Bandwidth and Kernel Choice,How does bandwidth selection affect the bias-variance tradeoff in kernel smoothing?,Larger bandwidth increases bias and reduces variance; smaller bandwidth decreases bias but increases variance.,Bandwidth does not affect bias or variance.,Larger bandwidth decreases bias and variance simultaneously.,Smaller bandwidth increases both bias and variance.,1
169,Bandwidth and Kernel Choice,What is the role of the kernel function in kernel smoothing?,It defines weights that decrease with distance from the target point.,It calculates the residuals.,It determines the number of neighbors to use.,It normalizes the predictors.,1
170,Bandwidth and Kernel Choice,Which factor is generally more critical for the performance of a kernel smoother?,The choice of bandwidth (window width).,The exact shape of the kernel function.,The color of the plot.,The programming language used.,1
171,Multidimensional Kernel Smoothing,What is multidimensional kernel smoothing?,Applying kernel smoothing techniques to data with multiple input features.,Clustering data into multiple groups.,Fitting a single polynomial in multiple dimensions.,Using kernel density estimation for 1-D data.,1
172,Multidimensional Kernel Smoothing,What major issue arises when performing kernel smoothing in many dimensions?,"The curse of dimensionality, requiring exponentially more data for reliable estimates.",It becomes a parametric method.,It automatically selects the best bandwidth.,It only works for one feature at a time.,1
173,Multidimensional Kernel Smoothing,Which kernel is commonly used for multivariate smoothing?,Gaussian radial basis function (RBF) kernel.,Decision tree kernel.,Linear basis kernel.,Polynomial kernel of degree 1.,1
174,Multidimensional Kernel Smoothing,How is bandwidth typically handled in multidimensional smoothing?,It may use a separate bandwidth for each dimension or a covariance matrix to define distance.,Bandwidth is ignored in multiple dimensions.,"Only one global bandwidth is used, same as in 1-D.",Bandwidth equals the number of dimensions.,1
175,Multidimensional Kernel Smoothing,What happens to the required sample size as the number of features increases in kernel smoothing?,The required sample size grows rapidly (exponentially) to maintain performance.,It decreases due to more information.,It remains the same.,It is always infinite.,1
176,Local Likelihood Estimation,What is local likelihood estimation?,A method that fits parametric models (like logistic regression) in local neighborhoods of the data.,Maximum likelihood estimation on the entire dataset globally.,A clustering technique for likelihood functions.,A technique unrelated to regression.,1
177,Local Likelihood Estimation,When would you use local likelihood methods?,"When the response distribution is not normal (e.g., binary or count data) and the relationship may vary with predictors.",Only when the data is normally distributed.,When there are no predictors.,When you want a single global estimate.,1
178,Local Likelihood Estimation,Which of the following is an example of local likelihood estimation?,Local logistic regression (fitting a logistic model in a moving window).,Principal component regression.,K-means clustering.,Support vector machine.,1
179,Local Likelihood Estimation,How does local likelihood differ from local least squares regression?,"Local likelihood can handle non-Gaussian outcomes by maximizing a weighted likelihood, not just squares of errors.",It only works for continuous outcomes.,It does not use weights.,It does not require any data.,1
180,Local Likelihood Estimation,For which type of outcome would local likelihood estimation be particularly useful?,Binary or count outcomes (allowing logistic or Poisson regression locally).,Any continuous outcome (there is no difference).,Only time-series data.,Only for text data.,1
181,Bootstrap Methods,What is the bootstrap method in statistics?,An approach that repeatedly resamples (with replacement) from the data to estimate the sampling distribution of a statistic.,A method for solving differential equations.,A type of clustering algorithm.,A parametric model fitting technique.,1
182,Bootstrap Methods,What can the bootstrap method be used to estimate?,"Confidence intervals and standard errors of almost any statistic (e.g., median, regression coefficients).",The exact analytical solution of the parameters.,Only the mean of a distribution.,The number of clusters in the data.,1
183,Bootstrap Methods,Which of these is an example of something the bootstrap can help compute?,The standard error of the sample median when no closed-form formula exists.,The maximum value of the sample.,The mean of the data (which is trivial).,The prior probability.,1
184,Bootstrap Methods,Which of the following describes how a bootstrap sample is drawn?,Randomly sampling data points with replacement from the original dataset to create a new dataset of the same size.,Randomly sampling without replacement (same as permutation).,Sampling only once from the data.,Using the entire dataset multiple times.,1
185,Bootstrap Methods,When is bootstrap especially useful?,When the theoretical sampling distribution of a statistic is complex or unknown.,When the data exactly follows a known distribution.,When there is no variance in the data.,When you have a parametric formula for all estimates.,1
186,Maximum Likelihood Estimation,What is maximum likelihood estimation (MLE)?,A method of estimating parameters by choosing values that maximize the likelihood of the observed data.,A method that minimizes the sum of squared errors.,A Bayesian inference technique.,A data sampling method.,1
187,Maximum Likelihood Estimation,When does ordinary least squares (OLS) coincide with MLE?,When the errors are normally distributed.,Never; they are completely different.,Only when there are two predictors.,When the data is non-numeric.,1
188,Maximum Likelihood Estimation,Which statement is true about MLE as the sample size grows large?,The MLE tends to be normally distributed around the true parameter (asymptotically normal).,The MLE always diverges.,The MLE ignores the data completely.,The MLE is exactly equal to the sample mean.,1
189,Maximum Likelihood Estimation,Maximizing the likelihood is often done by maximizing the log-likelihood because:,The log-likelihood is easier to compute and has the same maximizer as the likelihood.,The log-likelihood always has a closed-form solution.,The likelihood cannot be computed.,The log function is linear.,1
190,Maximum Likelihood Estimation,"Under standard conditions, what does the MLE converge to as the sample size increases?",The true parameter value (consistency).,Infinity.,Zero.,The prior mean.,1
191,Bayesian Inference Basics,What is the key idea behind Bayesian inference?,Updating prior beliefs with data using Bayes' theorem to obtain a posterior distribution of parameters.,Using cross-validation to tune parameters.,Maximizing the likelihood without priors.,A method unrelated to probability.,1
192,Bayesian Inference Basics,"According to Bayes' theorem, the posterior distribution is proportional to:",(Likelihood × Prior) of the parameters.,(Prior × Prior) of the parameters.,The inverse of the likelihood.,Random noise.,1
193,Bayesian Inference Basics,"In Bayesian terminology, what is the 'prior'?",The distribution representing our belief about the parameters before seeing the data.,The observed data itself.,The model prediction after training.,A type of likelihood function.,1
194,Bayesian Inference Basics,What is the 'posterior' in Bayesian inference?,The updated distribution of the parameters after observing the data.,The prior distribution after normalization.,The likelihood function itself.,The distribution of future data.,1
195,Bayesian Inference Basics,Which of the following describes a fundamental difference between Bayesian and frequentist inference?,Bayesian inference incorporates prior beliefs through probability distributions over parameters.,Frequentist inference always uses priors.,Bayesian inference ignores the data.,Frequentist inference provides a distribution over parameters.,1
196,"Prior, Posterior and Predictive Distribution",What is the predictive distribution in Bayesian inference?,"The probability distribution of a new observation given the existing data, integrating over the posterior of parameters.",The prior distribution after seeing data.,The likelihood of the parameters.,The distribution of just the new data.,1
197,"Prior, Posterior and Predictive Distribution",How do you compute the posterior predictive distribution for a new data point y_new?,Integrate the likelihood of y_new given parameters times the posterior of the parameters.,Multiply prior by likelihood of old data.,Take the difference between prior and posterior.,Use only the prior predictive.,1
198,"Prior, Posterior and Predictive Distribution",What does the posterior distribution represent?,The updated belief about model parameters after observing data.,The distribution of new data before seeing the data.,The probability of the prior distribution being correct.,The frequency of data values.,1
199,"Prior, Posterior and Predictive Distribution",What is the prior predictive distribution?,The distribution of data predicted by the model before observing any actual data (averaging over the prior).,The same as the posterior.,The likelihood function after data.,The final prediction after training.,1
200,"Prior, Posterior and Predictive Distribution",Which of the following is true about the posterior predictive distribution?,It accounts for uncertainty in model parameters by integrating over the posterior.,It ignores the posterior distribution.,It only depends on the prior.,It is the same as the likelihood.,1
201,EM Algorithm Overview,What is the purpose of the Expectation-Maximization (EM) algorithm?,To find maximum likelihood estimates when there is missing or latent data by iteratively applying expectation and maximization steps.,To perform gradient descent on any objective function.,To cluster data points without supervision.,To maximize the number of parameters in the model.,1
202,EM Algorithm Overview,What is done during the E-step of the EM algorithm?,Compute the expected complete-data log-likelihood given current parameter estimates.,Maximize the log-likelihood with respect to parameters.,Randomly sample missing data.,Evaluate the final model.,1
203,EM Algorithm Overview,What is done during the M-step of the EM algorithm?,Maximize the expected log-likelihood from the E-step to update the parameters.,Compute posterior probabilities of classes.,Discard the data and start over.,Calculate the residuals.,1
204,EM Algorithm Overview,Which property holds for the observed-data likelihood at each EM iteration?,The observed-data likelihood never decreases (it increases or stays the same).,The observed-data likelihood decreases.,The observed-data likelihood goes to zero.,The observed-data likelihood stays random.,1
205,EM Algorithm Overview,Which of the following is an example of using the EM algorithm?,Fitting a Gaussian mixture model with unknown component labels.,Linear regression on a fully observed dataset.,Randomly splitting data into training and test sets.,Hierarchical clustering.,1
206,Handling Missing Data,Which of the following is a common strategy for handling missing data?,"Imputing missing values (e.g., mean imputation or regression imputation).",Discarding the entire dataset.,Treating missing values as a separate category always.,Ignoring the problem completely.,1
207,Handling Missing Data,What is a drawback of simply removing all observations with missing values?,It can significantly reduce the dataset size and introduce bias if data are not missing completely at random.,It always improves model accuracy.,It introduces new missing values.,It only works for categorical data.,1
208,Handling Missing Data,How does multiple imputation address missing data?,By creating several different plausible imputed datasets and combining the results to reflect uncertainty.,By filling missing values with zeros only once.,By deleting all missing data points systematically.,By ignoring the missingness mechanism.,1
209,Handling Missing Data,Which method can incorporate missing data directly during training without explicit imputation?,Using model-based methods like EM that treat missing values as latent variables.,Using k-means clustering on incomplete data.,Taking the mean of observed values.,Using least squares ignoring missingness.,1
210,Handling Missing Data,"In the context of missing data, what does 'missing completely at random' (MCAR) mean?",The probability of missingness is independent of both observed and unobserved data values.,Missingness depends only on observed data.,Missingness depends on the missing values themselves.,Data cannot be missing at all.,1
211,Bagging and Model Averaging,What is bagging (bootstrap aggregating) in machine learning?,An ensemble method that builds multiple models on bootstrap samples and averages their predictions.,A single decision tree algorithm.,A technique to reduce number of features.,A method of clustering data.,1
212,Bagging and Model Averaging,What is the primary effect of bagging on model performance?,It reduces variance and helps prevent overfitting.,It increases bias dramatically.,It always makes the model underfit.,It converts regression into classification.,1
213,Bagging and Model Averaging,For which type of base learner is bagging particularly effective?,High-variance models (like decision trees).,Low-variance models (like linear regression).,Models with no parameters.,Reinforcement learning agents.,1
214,Bagging and Model Averaging,What does the 'bootstrap' part of bagging refer to?,Creating multiple datasets by sampling the original data with replacement.,Sampling each point exactly once.,Sampling parameters instead of data.,Using cross-validation.,1
215,Bagging and Model Averaging,How does bagging improve predictive accuracy?,By averaging multiple models' outputs to reduce variance and avoid overfitting.,By increasing the complexity of a single model.,By focusing only on the hardest examples.,By reducing the dataset size.,1
216,Bayesian Model Averaging,What is Bayesian Model Averaging (BMA)?,A technique that averages predictions from multiple models weighted by their posterior probabilities.,An algorithm for deep learning.,A method to cluster Bayesian networks.,A technique for data preprocessing.,1
217,Bayesian Model Averaging,How are models weighted in Bayesian model averaging?,By their posterior probabilities given the data (Bayes factors).,By their number of parameters.,"Equally, regardless of model performance.",By the order in which they were created.,1
218,Bayesian Model Averaging,What is a main advantage of model averaging over choosing a single model?,It accounts for model uncertainty and often yields more robust predictions.,It always selects the true model.,It eliminates the need for data.,It reduces the number of features.,1
219,Bayesian Model Averaging,Which of the following is true about Bayesian model averaging?,It uses the idea that combining multiple models can improve predictions by incorporating uncertainty.,It picks the model with the highest accuracy only.,It only works for linear regression.,It ignores prior information.,1
220,Bayesian Model Averaging,What does Bayesian model averaging require in order to combine models?,Calculating or approximating each model's posterior probability given the data.,Only the best model to have a nonzero weight.,No computation beyond standard averaging.,Renaming of features.,1
